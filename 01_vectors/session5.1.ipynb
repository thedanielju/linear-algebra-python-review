{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fe9b5a",
   "metadata": {},
   "source": [
    "# refresher\n",
    "\n",
    "dot products = a * b = a1b1 + a2b2 + .. aNbN\n",
    "geometrically expressed as a * b = ||a|| ||b|| cos(theta)\n",
    "measures how aligned they are sclaed by lengths - represents similarity and weighted\n",
    "sum\n",
    "\n",
    "comparing cosine similarities is more meaningful than comaring raw magnitude\n",
    "\n",
    "subtracting score.max prevents exp() from oveflowing by ensuring largest exponent is\n",
    "exp(0) = 1\n",
    "\n",
    "# real world example\n",
    "spam vs not spam\n",
    "\n",
    "each email is a vector of features\n",
    "0 = \"free\", 1 = \"buy now\", 2 = percent of all-caps, 3 = number of links\n",
    "\n",
    "stage 1: standardization (z-score)\n",
    "each feature might have different scales - links could be 0 - 50, caps could be 0 - 200, etc. without fixing one scale, one feature would dominate\n",
    "\n",
    "training mean and std are computed per feature\n",
    "z = x - mu / delta (how unusual is this email on each feature compared to normal training)\n",
    "\n",
    "a standardized vector = a vector where each feature has been turned into a z=score\n",
    "\n",
    "stage 2: logits (raw scores)\n",
    "each standardized vector produces scores per class\n",
    "\n",
    "scores[0] = not spam\n",
    "scores[1] = spam\n",
    "\n",
    "dot products generate scores\n",
    "\n",
    "stage 3: argmax (prediction)\n",
    "argmax(scores) = index of biggest score\n",
    "this index gives us the predicted class\n",
    "\n",
    "stage 4: softmax - gives the probability\n",
    "computed once per example (one email gets one probability vector)\n",
    "\n",
    "stage 5: cross entropy loss (training signal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a933e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5       1.6       1.5999999]\n"
     ]
    }
   ],
   "source": [
    "# let feature vector x have 3 features\n",
    "# mu, sigma for these 3 features\n",
    "# compute z-score\n",
    "# s_spam = w @ z + b\n",
    "# 2-class score vector, [0, s_spam] where 0 = not spam, 1 = spam\n",
    "# softmax to get probabilities\n",
    "# y to compute loss\n",
    "\n",
    "import numpy as np\n",
    "x = np.array([2, 1, 0.10], dtype=np.float32) #free occurs twice, one link, 10% of email is caps\n",
    "mu = np.array([0.5, 0.2, 0.02], dtype=np.float32)\n",
    "sigma = np.array([1.0, 0.5 ,0.05], dtype=np.float32)\n",
    "w = np.array([1.2, 0.7 ,1.5], dtype=np.float32)\n",
    "b = -0.3\n",
    "y = 1 # email is spam\n",
    "\n",
    "z = (x - mu) / sigma\n",
    "print(z) # [1.5       1.6       1.5999999] is about 1.5 std above average, strong indicator this is spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e21c53b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0199995\n",
      "[0.        5.0199995]\n"
     ]
    }
   ],
   "source": [
    "# logit - spam score for this vector\n",
    "s_spam = w @ z + b\n",
    "print(s_spam) # 5.0199995, has to be converted to probability\n",
    "\n",
    "scores = np.array([0.0, s_spam], dtype=np.float32)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a5bb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0065612 0.9934388]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# compute probability vectors\n",
    "shifted = scores - scores.max() # hedge against overflow\n",
    "\n",
    "exp = np.exp(shifted)\n",
    "prob = exp / exp.sum()\n",
    "print(prob) # the odds of this email being spam are 99% and not spam 0.66%\n",
    "print(prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e548cab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9934388\n",
      "0.006582839\n"
     ]
    }
   ],
   "source": [
    "# cross-entropy\n",
    "p_correct = prob[y]\n",
    "loss = -np.log(p_correct)\n",
    "print(p_correct)\n",
    "print(loss)\n",
    "# this tells us if the null hypothesis was true, how surprising is my data\n",
    "# how much probability did the model assign to the truth?\n",
    "\n",
    "# small = big loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a556941",
   "metadata": {},
   "source": [
    "for two choices, loss would be loss = -log(0.5) = 0.693.\n",
    "a loss close to 0.693 would mean closer to random\n",
    "\n",
    "loss tells us how wrong am i?\n",
    "later, gradient descent tells us how can we change the parameters to reduce loss\n",
    "through the derivatives of loss with respect to w and b to update them\n",
    "\n",
    "in many ML models, w and b start as random and are refined. these are weights in neural nets! w = weights, b = bias\n",
    "\n",
    "training data shapes these weights via optimization!\n",
    "\n",
    "# attribution is hard - saying which internal components represent to which abstractions\n",
    "# and why a specific output happens is the \"black box\". we DO know how training and inference work.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
