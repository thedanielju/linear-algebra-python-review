{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a841d4",
   "metadata": {},
   "source": [
    "# softmax - vector scores into a probability distribution\n",
    "\n",
    "every output is in (0, 1) - strictly positive\n",
    "all outputs sum to 1\n",
    "\n",
    "softmax(s_i) = e^(s_i) / sum of e^s_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "758e1068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01895101 0.00232067 0.513812   0.4649163 ]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "scores = np.array([0.1, -2.0, 3.4, 3.3], dtype=np.float32)\n",
    "exps = np.exp(scores)\n",
    "probs = exps / exps.sum()\n",
    "print(probs)\n",
    "print(probs.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b5791e",
   "metadata": {},
   "source": [
    "softmax = normalized scores into a probability distribution (sum = 1)\n",
    "exponentials preserve order and amplify gaps\n",
    "normalization lets us interpret these a probabilities\n",
    "\n",
    "SCORES ARE NOT Z-SCORES\n",
    "scores = model outputs (logits)\n",
    "\n",
    "# numerical stabilitiy:\n",
    "exponentials can overflow if scores are large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f3937f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf inf inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d8/b57xbrfd4llc4n5g_147z9c80000gn/T/ipykernel_61563/141589775.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  print(np.exp(big))\n"
     ]
    }
   ],
   "source": [
    "big = np.array([1000.0, 1001.0, 1002.0], dtype=np.float32)\n",
    "print(np.exp(big))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e243d5",
   "metadata": {},
   "source": [
    "# stability trick: \n",
    "subtract the max score before exponentiating - doesn't change the softmax result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09014b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472848 0.66524094]\n"
     ]
    }
   ],
   "source": [
    "shifted = big - big.max() # subtract max score from EACH ELEMENT in array\n",
    "\n",
    "# subtracting big max guarantees largest shifted value is 0\n",
    "\n",
    "exps = np.exp(shifted) # exponent of each element in new shifted array  \n",
    "probs = exps / exps.sum() # softmax \n",
    "print(probs) # will print identical probabilities, because subtracting the same constant\n",
    "# from every element cancels out the subtracted term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf16d7",
   "metadata": {},
   "source": [
    "# cross entropy loss\n",
    "\n",
    "1. what is a true class index?\n",
    "- classification involves fixed set of classes like\n",
    "    a. class 0: cat\n",
    "    b. class 1: dog\n",
    "    c. class 2: fox\n",
    "    d. class 3: bear\n",
    "- models output one score per class\n",
    "- the correct answer for training = true class index: label represented by an integer\n",
    "pointing to the correct position in the score vector\n",
    "\n",
    "2. softmax turns these raw scores into probabilities summing 1\n",
    "- tells us how confident a model is in each class\n",
    "\n",
    "3. log usage\n",
    "- we want a loss that gets smaller as the model assigns higher probabilities to the correct class\n",
    "- gets bigger when correct classes have low probbility\n",
    "- e.g: p(y) = model returns correct class\n",
    "    - py = 1, perfect\n",
    "    - py = 0.01, really bad\n",
    "- loss = -log(py)\n",
    "    - log(1) = 0; perfect prediction gives loss 0\n",
    "    - log(0.01) = big negative; big penalty loss\n",
    "\n",
    "can be used later in calculus for gradient based learning\n",
    "\n",
    "# loss = single number measuring how wrong the model is on an example\n",
    "training is just making loss smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "294ab349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01895101 0.00232067 0.513812   0.4649163 ]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "scores = np.array([0.1, -2.0, 3.4, 3.3], dtype=np.float32)\n",
    "y = 2\n",
    "\n",
    "shifted = scores - scores.max()\n",
    "probs = np.exp(shifted) / sum(np.exp(shifted))\n",
    "print(probs)\n",
    "print(probs.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451280cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.513812 0.66589785\n"
     ]
    }
   ],
   "source": [
    "p_correct = probs[y] # access correct value, store\n",
    "loss = -np.log(p_correct) # gives us loss - different value per example\n",
    "print(p_correct, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7bf62",
   "metadata": {},
   "source": [
    "# loss is like damage points\n",
    "every training example hits model w/ some damage based on how \"wrong\" it was\n",
    "\n",
    "p_correct is a probability post softmax\n",
    "loss will only compare different probabilities 0-1\n",
    "-log(p_correct) converts probabilitiy into an inversely related penalty (one input, model outputs probabilities for each class, p_correct = probability of sitting at the correct value in this probability vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a44bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7310586\n"
     ]
    }
   ],
   "source": [
    "scores_big = scores * 10\n",
    "shifted_big = scores_big - scores_big.max()\n",
    "probs = np.exp(shifted_big)/sum(np.exp(shifted_big))\n",
    "p_correct = probs[2]\n",
    "print(p_correct) # scaling logits up made softmax more confident\n",
    "\n",
    "# temperature in disguise: bigger scale (lower temperature) makes \n",
    "# probabilities more peaked, smaller scale makes them flatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2067a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
